import json
from pathlib import Path

import pytest
from testing_utils import CargoTools

FAILED_CONFIGS = []


# Cmdline options
def pytest_addoption(parser):
    parser.addoption(
        "--traces",
        choices=["none", "target", "all"],
        default="none",
        help="Verbosity of traces in output and HTML report. "
        '"none" - show no traces, '
        '"target" - show traces generated by test code, '
        '"all" - show all traces. ',
    )
    parser.addoption(
        "--target-path",
        type=Path,
        help="Path to test scenarios executable. Search is performed by default.",
    )
    parser.addoption(
        "--target-name",
        type=str,
        default="rust_test_scenarios",
        help='Test scenario executable name. Overwritten by "--target-path".',
    )
    parser.addoption(
        "--build-scenarios",
        action="store_true",
        help="Build test scenarios executables.",
    )
    parser.addoption(
        "--build-scenarios-timeout",
        type=float,
        default=180.0,
        help="Build command timeout in seconds. Default: %(default)s",
    )
    parser.addoption(
        "--default-execution-timeout",
        type=float,
        default=5.0,
        help="Default execution timeout in seconds. Default: %(default)s",
    )


# Hooks
@pytest.hookimpl(tryfirst=True)
def pytest_sessionstart(session):
    try:
        # Build scenarios.
        if session.config.getoption("--build-scenarios"):
            print("Building test scenarios executable...")
            build_timeout = session.config.getoption("--build-scenarios-timeout")
            tools = CargoTools(build_timeout=build_timeout)
            target_name = tools.select_target_path(
                session.config, expect_exists=False
            ).name
            tools.build(target_name)

    except Exception as e:
        pytest.exit(str(e), returncode=1)


def pytest_html_report_title(report):
    # Change report title
    report.title = "Component Integration Tests Report"


def pytest_html_results_table_header(cells):
    # Create additional table headers
    cells.insert(1, "<th>Test Input</th>")
    cells.insert(2, "<th>Description</th>")
    cells.insert(3, "<th>Test Scenario Name</th>")
    cells.insert(4, "<th>Test Scenario Command</th>")


def pytest_html_results_table_row(report, cells):
    # Create additional table columns with TC __doc__ and execution date
    cells.insert(
        1,
        f'<td><pre style="white-space:pre-wrap;word-wrap:break-word">{json.dumps(report.input)}</pre></td>',
    )
    cells.insert(2, f"<td><pre>{report.description}</pre></td>")
    cells.insert(3, f"<td><pre>{report.scenario}</pre></td>")
    cells.insert(4, f"<td><pre>{report.command}</pre></td>")


@pytest.hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item, call):
    # Extract TC's data
    outcome = yield
    report = outcome.get_result()
    report.description = str(item.function.__doc__)
    report.scenario = item.funcargs.get("scenario_name", "")
    report.input = item.funcargs.get("test_config", "")

    command = []
    for token in item.funcargs.get("command", ""):
        if " " in token:
            command.append(f"'{token}'")
        else:
            command.append(token)
    report.command = " ".join(command)

    # Store failed command for printing in summary
    if report.failed:
        FAILED_CONFIGS.append(
            {
                "nodeid": report.nodeid,
                "command": report.command,
            }
        )


def pytest_terminal_summary(terminalreporter):
    if not FAILED_CONFIGS:
        return
    # Print failed scenarios info
    terminalreporter.write_sep("=", "Failed tests reproduction info")
    terminalreporter.write_line(
        "Run failed scenarios from the repo root working directory\n"
    )

    for entry in FAILED_CONFIGS:
        terminalreporter.write_line(
            f"{entry['nodeid']} | Run command:\n{entry['command']}\n"
        )
